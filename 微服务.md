# 分布式与微服务 SpringCloud SpringCloudAlibaba

SpringCloud是Spring提供的一套分布式解决方案，集合了一些大型互联网公司的开源产品，包括诸多组件，共同组成SpringCloud框架。

### 微服务的优势：

1. 传统单体应用架构将所有功能都整合在一个应用之中，导致整体应用十分庞大、代码复杂，不利用扩展。
2. 单体架构将所有功能放在一个应用中，当用户流量大了之后，服务器可能无法承受住流量，而使用微服务，将各种功能分开来，分别部署到不同的服务器上，可以减轻服务器的负担。
3. 同时，同一个微服务可以有多个，分别进行部署，这样可以保证服务的高可用性
4. 微服务之间可以通过不同的协议进行调用，这样服务的实现方式也可以有所不同

### 微服务CAP原则

> CAP原则又称CAP定理，指的是在一个分布式系统中，存在Consistency（一致性）、Availability（可用性）、Partition tolerance（分区容错性），三者不可同时保证，最多只能保证其中的两者。

一致性（C）：在分布式系统中的所有数据备份，在同一时刻都是同样的值（所有的节点无论何时访问都能拿到最新的值）

可用性（A）：系统中非故障节点收到的每个请求都必须得到响应（比如我们之前使用的服务降级和熔断，其实就是一种维持可用性的措施，虽然服务返回的是没有什么意义的数据，但是不至于用户的请求会被服务器忽略）

分区容错性（P）：一个分布式系统里面，节点之间组成的网络本来应该是连通的，然而可能因为一些故障（比如网络丢包等，这是很难避免的），使得有些节点之间不连通了，整个网络就分成了几块区域，，这时也需要能够对外提供服务。即系统中任意信息的丢失或失败不会影响系统的继续运作

**总的来说，数据存放的节点数越多，分区容忍性就越高，但是要复制更新的次数就越多，一致性就越难保证。同时为了保证一致性，更新所有节点数据所需要的时间就越长，那么可用性就会降低。**

#### AC 可用性+一致性

要同时保证可用性和一致性，代表着某个节点数据更新之后，需要立即将结果通知给其他节点，并且要尽可能的快，这样才能及时响应保证可用性，这就对网络的稳定性要求非常高，但是实际情况下，网络很容易出现丢包等情况，并不是一个可靠的传输，如果需要避免这种问题，就只能将节点全部放在一起，但是这显然违背了分布式系统的概念，所以对于我们的分布式系统来说，很难接受。

#### CP 一致性+分区容错性

为了保证一致性，那么就得将某个节点的最新数据发送给其他节点，并且需要等到所有节点都得到数据才能进行响应，同时有了分区容错性，那么代表我们可以容忍网络的不可靠问题，所以就算网络出现卡顿，那么也必须等待所有节点完成数据同步，才能进行响应，因此就会导致服务在一段时间内完全失效，所以可用性是无法得到保证的。

#### AP 可用性+分区容错性

既然CP可能会导致一段时间内服务得不到任何响应，那么要保证可用性，就只能放弃节点之间数据的高度统一，也就是说可以在数据不统一的情况下，进行响应，因此就无法保证一致性了。虽然这样会导致拿不到最新的数据，但是只要数据同步操作在后台继续运行，一定能够在某一时刻完成所有节点数据的同步，那么就能实现**最终一致性**，所以AP实际上是最能接受的一种方案。

比如我们实现的Eureka集群，它使用的就是AP方案，Eureka各个节点都是平等的，少数节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka客户端在向某个Eureka服务端注册时如果发现连接失败，则会自动切换至其他节点。只要有一台Eureka服务器正常运行，那么就能保证服务可用，只不过查询到的信息可能不是最新的

### 服务注册、服务发现：

### 流量限制：

我们的机器不可能无限制的接受和处理客户端的请求，如果不加以限制，当发生高并发情况时，系统资源将很快被耗尽。为了避免这种情况，我们就可以添加流量控制，当一段时间内的流量到达一定的阈值的时候，新的请求将不再进行处理，这样不仅可以合理地应对高并发请求，同时也能在一定程度上保护服务器不受到外界的恶意攻击。

**如果判断流量超过阈值呢**？

1. **漏桶算法**
   顾名思义，就像一个桶开了一个小孔，水流进桶中的速度肯定是远大于水流出桶的速度的，这也是最简单的一种限流思路：
   ![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h0okai0crij21om08kgmz.jpg)
   我们知道，桶是有容量的，所以当桶的容量已满时，就装不下水了，这时就只有丢弃请求了（不处理)。

2. **令牌桶算法**
   现在有一个令牌桶，这个桶是专门存放令牌的，每隔一段时间就向桶中丢入一个令牌（速度由我们指定）当新的请求到达时，将从桶中删除令牌，接着请求就可以通过并给到服务，但是如果桶中的令牌数量不足，那么不会删除令牌，而是让此数据包等待。
   ![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h0okow2vd5j21lg0gkdid.jpg)
   可以试想一下，当流量下降时，令牌桶中的令牌会逐渐积累，这样如果突然出现高并发，那么就能在短时间内拿到大量的令牌。 （这时又会出现高并发问题)

3. **固定时间窗口算法**
   我们可以对某一个时间段内的请求进行统计和计数，比如在`14:15`到`14:16`这一分钟内，请求量不能超过`100`，也就是一分钟之内不能超过`100`次请求，那么就可以像下面这样进行划分：
   ![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h0okvim48fj219404274n.jpg)
   当遇到临界点时，固定时间窗口算法存在安全隐患。因为窗口的界限是固定的。 

4. **滑动时间窗口算法**
   相对于固定窗口算法，滑动时间窗口算法更加灵活，它会动态移动窗口，重新进行计算：
   ![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h0ol4irdckj21fs0jggnr.jpg)
   虽然这样能够避免固定时间窗口的临界问题，但是这样显然是比固定窗口更加耗时的。 

**限流的策略也有很多种：**

- 方案一：**快速拒绝**，既然不再接受新的请求，那么我们可以直接返回一个拒绝信息，告诉用户访问频率过高。

- 方案二：**预热**，依然基于方案一，但是由于某些情况下高并发请求是在某一时刻突然到来，我们可以缓慢地将阈值提高到指定阈值，形成一个缓冲保护。

- 方案三：**排队等待**，不接受新的请求，但是也不直接拒绝，而是进队列先等一下，如果规定时间内能够执行，那么就执行，要是超时就算了。

### 服务熔断与降级

服务降级是指当请求一个微服务，此微服务处于异常状态无法处理请求时，也要有一个备用的处理方法，快速处理请求返回。

服务熔断是指当达到某一个阈值后，微服务不再处理请求，全部转为降级处理，等过了一段时间，尝试恢复，如果成功则关闭熔断器，正常使用。

#### 熔断器状态变化

- 关闭：熔断器不工作，所有请求全部该干嘛干嘛。

- 打开：熔断器工作，所有请求一律降级处理。

- 半开：尝试进行一下下正常流程，要是还不行继续保持打开状态，否则关闭。

![image-20220804210605571](C:\Users\10441\AppData\Roaming\Typora\typora-user-images\image-20220804210605571.png)

### 分布式事务

在单体应用下，数据库事务可以简单的通过springboot的注解实现。

但是在微服务的背景下，却没有那么简单了。

**在多服务、多数据源的情况下，原来的解决方案会失效**

试想一下，在分布式环境下，有可能出现这样一个问题，比如我们下单购物，那么整个流程可能是这样的：先调用库存服务对库存进行减扣 -> 然后订单服务开始下单 -> 最后用户账户服务进行扣款，虽然看似是一个很简单的一个流程，但是如果没有事务的加持，很有可能会由于中途出错，比如整个流程中订单服务出现问题，那么就会导致库存扣了，但是实际上这个订单并没有生成，用户也没有付款。

![image-20220804162103980](C:\Users\10441\AppData\Roaming\Typora\typora-user-images\image-20220804162103980.png)

#### 分布式事务解决方案：

1. **XA分布式事务协议 - 2PC（两阶段提交实现）**
   这里的PC实际上指的是Prepare和Commit，也就是说它分为两个阶段，一个是准备一个是提交，整个过程的参与者一共有两个角色，一个是事务的执行者，一个是事务的协调者，实际上整个分布式事务的运作都需要依靠协调者来维持：
   ![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h0teawzsnnj21hc0duabp.jpg)
   在准备和提交阶段，会进行： 
- - **准备阶段：**
    一个分布式事务是由协调者来开启的，首先协调者会向所有的事务执行者发送事务内容，等待所有的事务执行者答复。
    各个事务执行者开始执行事务操作，但是不进行提交，并将undo和redo信息记录到事务日志中。
    如果事务执行者执行事务成功，那么就告诉协调者成功Yes，否则告诉协调者失败No，不能提交事务。 

- - **提交阶段：**
    当所有的执行者都反馈完成之后，进入第二阶段。
    协调者会检查各个执行者的反馈内容，如果所有的执行者都返回成功，那么就告诉所有的执行者可以提交事务了，最后再释放锁资源。
    如果有至少一个执行者返回失败或是超时，那么就让所有的执行者都回滚，分布式事务执行失败。 

虽然这种方式看起来比较简单，但是存在以下几个问题： 

- - 事务协调者是非常核心的角色，一旦出现问题，将导致整个分布式事务不能正常运行。

- - 如果提交阶段发生网络问题，导致某些事务执行者没有收到协调者发来的提交命令，将导致某些执行者提交某些执行者没提交，这样肯定是不行的。
2. **XA分布式事务协议 - 3PC（三阶段提交实现）**
   三阶段提交是在二阶段提交基础上的改进版本，主要是加入了超时机制，同时在协调者和执行者中都引入了超时机制。
   三个阶段分别进行： 
- - **CanCommit阶段：**
    协调者向执行者发送CanCommit请求，询问是否可以执行事务提交操作，然后开始等待执行者的响应。
    执行者接收到请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态，否则返回No 

- - **PreCommit阶段：**
    协调者根据执行者的反应情况来决定是否可以进入第二阶段事务的PreCommit操作。
    如果所有的执行者都返回Yes，则协调者向所有执行者发送PreCommit请求，并进入Prepared阶段，执行者接收到请求后，会执行事务操作，并将undo和redo信息记录到事务日志中，如果成功执行，则返回成功响应。
    如果所有的执行者至少有一个返回No，则协调者向所有执行者发送abort请求，所有的执行者在收到请求或是超过一段时间没有收到任何请求时，会直接中断事务。 

- - **DoCommit阶段：**
    该阶段进行真正的事务提交。
    协调者接收到所有执行者发送的成功响应，那么他将从PreCommit状态进入到DoCommit状态，并向所有执行者发送doCommit请求，执行者接收到doCommit请求之后，开始执行事务提交，并在完成事务提交之后释放所有事务资源，并最后向协调者发送确认响应，协调者接收到所有执行者的确认响应之后，完成事务（如果因为网络问题导致执行者没有收到doCommit请求，执行者会在超时之后直接提交事务，虽然执行者只是猜测协调者返回的是doCommit请求，但是因为前面的两个流程都正常执行，所以能够在一定程度上认为本次事务是成功的，因此会直接提交）
    协调者没有接收至少一个执行者发送的成功响应（也可能是响应超时），那么就会执行中断事务，协调者会向所有执行者发送abort请求，执行者接收到abort请求之后，利用其在PreCommit阶段记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源，执行者完成事务回滚之后，向协调者发送确认消息， 协调者接收到参与者反馈的确认消息之后，执行事务的中断。 

相比两阶段提交，三阶段提交的优势是显而易见的，当然也有缺点： 

- - 3PC在2PC的第一阶段和第二阶段中插入一个准备阶段，保证了在最后提交阶段之前各参与节点的状态是一致的。

- - 一旦参与者无法及时收到来自协调者的信息之后，会默认执行Commit，这样就不会因为协调者单方面的故障导致全局出现问题。

- - 但是我们知道，实际上超时之后的Commit决策本质上就是一个赌注罢了，如果此时协调者发送的是abort请求但是超时未接收，那么就会直接导致数据一致性问题。
3. **TCC（补偿事务）**
   补偿事务TCC就是Try、Confirm、Cancel，它对业务有侵入性，一共分为三个阶段
- - **Try阶段：**
    比如我们需要在借书时，将书籍的库存`-1`，并且用户的借阅量也`-1`，但是这个操作，除了直接对库存和借阅量进行修改之外，还需要将减去的值，单独存放到冻结表中，但是此时不会创建借阅信息，也就是说只是预先把关键的东西给处理了，预留业务资源出来。 

- - **Confirm阶段：**
    如果Try执行成功无误，那么就进入到Confirm阶段，接着之前，我们就该创建借阅信息了，只能使用Try阶段预留的业务资源，如果创建成功，那么就对Try阶段冻结的值，进行解冻，整个流程就完成了。当然，如果失败了，那么进入到Cancel阶段。 

- - **Cancel阶段：**
    不用猜了，那肯定是把冻结的东西还给人家，因为整个借阅操作压根就没成功。就像你付了款买了东西但是网络问题，导致交易失败，钱不可能不还给你吧。 

跟XA协议相比，TCC就没有协调者这一角色的参与了，而是自主通过上一阶段的执行情况来确保正常，充分利用了集群的优势，性能也是有很大的提升。但是缺点也很明显，它与业务具有一定的关联性，需要开发者去编写更多的补偿代码，同时并不一定所有的业务流程都适用于这种形式。 

### 分布式权限校验

Oauth2 

### 分布式数据库

#### 主从复制

主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(Master)，后者称为从节点(Slave)，数据的复制是单向的，只能由主节点到从节点。Master以写为主，Slave 以读为主。

##### 好处：

- 实现了读写分离，提高了性能。

- 在写少读多的场景下，我们甚至可以安排很多个从节点，这样就能够大幅度的分担压力，并且就算挂掉一个，其他的也能使用。

#### redis分布式

##### 主从复制

相关命令：`replicaof ip:port`

**重要参数：主从数据库的偏移量**

> 主服务器和从服务器都会维护一个复制偏移量，主服务器每次向从服务器中传递 N 个字节的时候，会将自己的复制偏移量加上 N。从服务器中收到主服务器的 N 个字节的数据，就会将自己额复制偏移量加上 N，通过主从服务器的偏移量对比可以很清楚的知道主从服务器的数据是否处于一致，如果不一致就需要进行增量同步了。

主节点关闭后，从节点依然可以读取数据，但是会报错。

###### 主从数据库的同步流程：

1. 从节点执行replicaof ip port命令后，从节点会保存主节点相关的地址信息。

2. 从节点通过每秒运行的定时任务发现配置了新的主节点后，会尝试与该节点建立网络连接，专门用于接收主节点发送的复制命令。

3. 连接成功后，第一次会将主节点的数据进行全量复制，之后采用增量复制，持续将新来的写命令同步给从节点。

但是并不是所有的从节点都必须直接连到主节点上，这回增加主节点同步数据的开销，影响性能，所以可以让部分从节点连接到其他从节点，从从节点那里同步数据，但是这也是有缺点的，如果即如果传播链路上一个节点挂了，会影响后续很多节点。

##### 哨兵模式

主从复制模式中，如果主节点出问题，那么整个系统将无法写入数据，所以引入了若干的哨兵节点，来管理这个系统，在主节点故障时，选举出新的主节点。

`sentinel monitor lbwnb 127.0.0.1 6001 1` 表示是哨兵节点，最后一个参数代表多少个哨兵认为主节点下线才会进行选举

**选举规则**:

1. 首先会根据优先级进行选择，可以在配置文件中进行配置，添加`replica-priority`配置项（默认是100），越小表示优先级越高。

2. 如果优先级一样，那就选择偏移量最大的(代表数据一致性最高)

3. 要是还选不出来，那就选择runid（启动时随机生成的）最小的。

*使用哨兵模式后，java客户端对redis的访问通过哨兵来配置。*

##### 集群模式

一台redis服务器不够存储所有数据，那么就可以把这些数据存放到多个redis数据库中，构建一个redis集群

![image-20220807205023296](C:\Users\10441\AppData\Roaming\Typora\typora-user-images\image-20220807205023296.png)

一个Redis集群包含16384个插槽，集群中的每个Redis 实例负责维护一部分插槽以及插槽所映射的键值数据。

实际上，插槽就是键的Hash计算后的一个结果，注意这里出现了`计算机网络`中的CRC循环冗余校验，这里采用CRC16，能得到16个bit位的数据，也就是说算出来之后结果是0-65535之间，再进行取模，得到最终结果：

**Redis key的路由计算公式：slot = CRC16（key） % 16384**

结果的值是多少，就应该存放到对应维护的Redis下，比如Redis节点1负责0-25565的插槽，而这时客户端插入了一个新的数据`a=10`，a在Hash计算后结果为666，那么a就应该存放到1号Redis节点中。简而言之，本质上就是通过哈希算法将插入的数据分摊到各个节点的。

**如果集群中某个主结点挂掉了，那么从节点会被选取出来成为主节点，如果没有任何从节点了，那么集群就不可用了。**

##### 分布式锁

分布式锁是是用来解决在分布式系统中的并发问题的。redis可以作为分布式锁实现的一种方式。

`setnx key value` 表示这个key只有在不存在时才能被创建，否则失败

那么这个key就可以看做是一个锁的概念

考虑并发编程的有限等待，setnx也可以设置超时时间，当一个key超过时间还没有被删除时，自动删除，防止别的服务一直等待锁

当然，添加了过期时间，带了的好处是显而易见的，但是同时也带来了很多的麻烦，我们来设想一下这种情况：

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h1935d9f0jj21im0cydi8.jpg)

因此，单纯只是添加过期时间，会出现这种把别人加的锁谁卸了的情况，要解决这种问题也很简单，我们现在的目标就是保证任务只能删除自己加的锁，如果是别人加的锁是没有资格删的，所以我们可以吧a的值指定为我们任务专属的值，比如可以使用UUID之类的，如果在主动删除锁的时候发现值不是我们当前任务指定的，那么说明可能是因为超时，其他任务已经加锁了。

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h193cc7opzj21hy0da40t.jpg)

这里还有一个问题，就是说其实检查值是否是原值与删除并不是一个原子的操作，如果在超时之前那一刹那进入到释放锁的阶段，获取到值肯定还是自己，但是在即将执行删除之前，由于超时机制导致被删除并且其他任务也加锁了，那么这时再进行删除，仍然会导致删除其他任务加的锁。

![img](https://tva1.sinaimg.cn/large/e6c9d24ely1h193j2b68fj21mw0d6mzv.jpg)

实际上本质还是因为锁的超时时间不太好衡量，如果超时时间能够设定地比较恰当，那么就可以避免这种问题了。

###### redisson

一个用redis实现分布式锁的解决方案。

它是Redis官方推荐的Java版的Redis客户端。它提供的功能非常多，也非常强大，Redisson内部提供了一个监控锁的看门狗，它的作用是在Redisson实例被关闭前，不断的延长锁的有效期，它为我们提供了很多种分布式锁的实现，使用起来也类似我们在JUC中学习的锁

注意，如果用于存放锁的Redis服务器挂了，那么肯定是会出问题的，这个时候我们就可以使用RedLock，它的思路是，在多个Redis服务器上保存锁，只需要超过半数的Redis服务器获取到锁，那么就真的获取到锁了，这样就算挂掉一部分节点，也能保证正常运行

#### MySQL数据库分布式

##### mysql的主从复制

使用MySQL的时候，也可以采取主从复制的策略，它的实现思路基本和Redis相似，也是采用增量复制的方式，MySQL会在运行的过程中，会记录二进制日志，所有的DML和DDL操作都会被记录进日志中，主库只需要将记录的操作复制给从库，让从库也运行一次，那么就可以实现主从复制。但是注意**它不会在一开始进行全量复制，所以最好再开始主从之前将数据库的内容保持一致。**

###### 从库的两个重要线程：

从库的状态中，Replica_IO_Running和Replica_SQL_Running必须同时为Yes才可以，实际上从库会创建两个线程，一个线程负责与主库进行通信，获取二进制日志，暂时存放到一个中间表（Relay_Log）中，而另一个线程则是将中间表保存的二进制日志的信息进行执行，然后插入到从库中。

注意：不同于redis，mysql的主库宕机，从库不会被选举，从库需要等待主库恢复

##### 分库分表

即将数据存放到不同的数据库、同一张表的数据拆分到不同的几张表里面去存储。

#### 分布式序列算法

保证主键id能够在分布式系统中不重复、比较有序的自动生成的算法。

##### 雪花算法：

![image-20220807212639853](C:\Users\10441\AppData\Roaming\Typora\typora-user-images\image-20220807212639853.png)

它主要是三个部分组成，时间+工作机器ID+序列号，时间以毫秒为单位，41个bit位能表示约70年的时间，时间纪元从2016年11月1日零点开始，可以使用到2086年，工作机器ID其实就是节点ID，每个节点的ID都不相同，那么就可以区分出来，10个bit位可以表示最多1024个节点，最后12位就是每个节点下的序列号，因此每台机器每毫秒就可以有4096个系列号。

##### uuid（不有序）

### 消息队列

是一种常用的消息中间件，可以作为一种发送方与接受方之间的消息缓冲区，主要作用是削峰填谷、解耦、日志记录（异步）

#### 消息队列的几种协议：

JMS:队列点对点、主题-发布订阅

AMQP：跨语言的消息队列协议支持事务，数据一致性高

三大实体 

- 队列
- 交换机（信箱）：消息先通过通道发送到信箱，再通过绑定的路由转发到队列
- 绑定

#### RabbitMq

架构图：

![image-20220807213315145](C:\Users\10441\AppData\Roaming\Typora\typora-user-images\image-20220807213315145.png)

- **生产者（Publisher）和消费者（Consumer）：**发送与获取消息的客户端。

- **Channel信道：**我们的客户端连接都会使用一个Channel，再通过Channel去访问到RabbitMQ服务器，注意通信协议不是http，而是amqp协议。信道是建立在TCP上面的虚拟链接，也就是[rabbitMQ](https://so.csdn.net/so/search?q=rabbitMQ&spm=1001.2101.3001.7020)在一个TCP上面建立成百上千的信道来达到多个线程处理。**注意是一个TCP 被多个线程共享，每个线程对应一个信道，信道在rabbit都有唯一的ID，保证了信道的私有性，对应上唯一的线程使用。**

- **Exchange：**类似于交换机一样的存在，会根据我们的请求，转发给相应的消息队列，每个队列都可以绑定到Exchange上，这样Exchange就可以将数据转发给队列了，可以存在很多个，不同的Exchange类型可以用于实现不同消息的模式。（根据routingKey路由到制定队列）

- **Queue：**消息队列本体，生产者所有的消息都存放在消息队列中，等待消费者取出。

- **Virtual Host：**有点类似于环境隔离，不同环境都可以单独配置一个Virtual Host，每个Virtual Host可以包含很多个Exchange和Queue，每个Virtual Host相互之间不影响。

##### 常见的消息队列模型：

###### 点对点：

​     即一个生产者，一个消费者

###### 工作队列模式：

​    一个生产者，多个消费者消费队列中的消息（涉及到消息分发策略以及队列中初始消息的分发策略）

###### 发布订阅模式：

​    如果某一条消息，需要各发送给多个消费者。这里需要用到另一种类型的交换机，叫做`fanout`（扇出）类型

###### 主题模式：将`routingKey`以模糊匹配的方式去进行转发。

​    `#` 代表0-多个单词 `*`代表一个任意单词

###### 第四种交换机类型

​    在头部信息中指定路由

##### 消费者消费消息的方式：（ACK MODE）

- Nack message requeue true：拒绝消息，也就是说不会将消息从消息队列取出，并且重新排队，一次可以拒绝多个消息。

- Ack message requeue false：确认应答，确认后消息会从消息队列中移除，一次可以确认多个消息。

- Reject message requeue true/false：也是拒绝此消息，但是可以指定是否重新排队。

##### 死信队列

超时、队列满、或者接受失败且不重新进入队列的消息会进入死信队列里面保存。

死信队列的使用需要指定死信队列以及死信交换机

#### Kafka

##### 基本概念：

- broker: 即一台服务器
- topic 主题：类比数据库中的表，相同类别消息存放到一个主题
- 分区：数据存放的区域，分区可以分不到不同的broker上。每个分区有一个offset，表明消息消费的位置。分区可以理解为一个有序、不可变的提交日志。
- 从节点副本：主分区的副本，每个分区都可以有副本，并且分布在不同的服务器上。读写分离
- segment段，分区内数据的基本存储单位
- 生产者，生成消息，发送到对应的主题的分区(设置消息的key)
- 消费者：从分区消费数据
- 消费者组：同一个消费者组的消费者不能消费同一个分区的消息。（减少锁的性能开销）,不同组的消费者可以消费同一个分区的消息，且是独立的(因为每一个消费者都有一个偏移量)。

##### 消息模式：

点对点

发布-订阅

##### 消息的消费顺序：

同一个分区里的消息顺序消费，不同分区的消息不保证顺序消费。即在同一个分区，先发来的消息的偏移量比后发来的大。

##### 生产者客户端的重要参数：acks

该参数表示生产者需要接收来自服务端的 ack 确认,当没有接受到确认或者超时时，会抛出异常，让客户端做进一步处理。

**request.required.acks = 0** 表示 Producer 不等待来自 Leader 的 ACK 确认，直接发送下一条消息。在这种情况下，如果 Leader 分片所在服务器发生宕机，那么这些已经发送的数据会丢失。

**request.required.acks = 1** 表示 Producer 等待来自 Leader 的 ACK 确认，当收到确认后才发送下一条消息。在这种情况下，消息一定会被写入到 Leader 服务器，但并不保证 Follow 节点已经同步完成。所以如果在消息已经被写入 Leader 分片，但是还未同步到 Follower 节点，此时Leader 分片所在服务器宕机了，那么这条消息也就丢失了，无法被消费到。

**request.required.acks = -1** (all)表示 Producer 等待来自 Leader 和所有 Follower 的 ACK 确认之后，才发送下一条消息。在这种情况下，除非 Leader 节点和所有 Follower 节点都宕机了，否则不会发生消息的丢失。（在引入ISR机制后，leader只需要跟isr中的结点完成同步即可返回确认了[isr结点的判断标准是落后leader结点的最长时间间隔]）

##### 消息传递语义：

- 最多一次——消息可能会丢失，永远不重复发送

- 最少一次——消息不会丢失，但是可能会重复

- 精确一次——保证消息被传递到服务端且在服务端不重复

消息的传递语义需要生产者与消费者共同来保证

**如何保证精确一次呢？**

首先生产者方，需要开启幂等性配置，并且acks设置为-1、重试次数可以设置为最大整数，这样就能保证生产者发送消息精确一次。如果发送到多分区，还需要开启事务。

然后是消费者方，需要自己处理这方面的逻辑，关闭autoCommit，手动提交offset，

还有一个选择就是使用kafka自己的流处理引擎，也就是Kafka Streams，

设置processing.guarantee=exactly_once，就可以轻松实现exactly once了。

##### 事务：

指一系列消息操作要么都成功，要么都不成功。

比如说消息要提交到多分区，这时候可以设置事务，让所有分区要么都成功提交，要么都失败。

或者一系列消息的提交，其中有一个失败了，则都失败，也可以用事务来实现，此时消息的状态位uncommited

###### 事务隔离级别：

读未提交（脏读）：对于那些状态为未提交的数据，也可以消费

读已提交：只能消费已提交的消息。
